library(dplyr)
library(rvest)
library(rlang)
library(furrr)
library(doParallel)
library(foreach)
library(stringr)
library(dplyr)
library(rvest)
library(rlang)
library(furrr)
library(doParallel)
library(foreach)
library(stringr)
Sys.setenv(http_proxy="http://proxyserver:port")
#This chunk and the on bellow it helps set up parallel processing. If you have have enough time in theory this does not need to be done in parallel
parallel::detectCores()
n.cores <- parallel::detectCores() - 1
#create the cluster
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#check cluster definition (optional)
print(my.cluster)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
#how many workers are available? (optional)
foreach::getDoParWorkers()
#create the cluster
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#check cluster definition (optional)
print(my.cluster)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
#how many workers are available? (optional)
foreach::getDoParWorkers()
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
y=read_html(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href") #
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
#this part was limiting the contracts just down to the state level, as country wide analysis didn't make sense for the independent project
generaldynamicsMD <- foreach(i = 1994:2022, .combine = 'rbind') %dopar% {
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste('https://www.fpds.gov/ezsearch/fpdsportal?indexName=awardfull&templateName=1.5.3&s=FPDS.GOV%252525252525252525252525252525252BtemplateName1.5.2%252525252525252525252525252525252BindexNameawardfull&q=ULTIMATE_UEI_NAME%3A%22GENERAL+DYNAMICS%22+CONTRACT_FISCAL_YEAR%3A%22',as.character(i),'%22+VENDOR_ADDRESS_STATE_CODE%3A%22MD%22&x=24&y=13',sep=""))
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
x<- read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
str_sub(x,-5,-1)
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=read_url(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href") #
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
x<- read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
str_sub(x,-5,-1)
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=read_url(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href") #
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
x<- read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
str_sub(x,-5,-1)
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href") #
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href") #
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep=""))
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- foreach(i = 1979:2022, .combine = 'rbind') %dopar% { #the reason why a loop is used, and sort of why this needs to be data scraped in the first place is because the website caps each csv at 50,000 observations
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste("https://www.fpds.gov/ezsearch/fpdsportal?q=VENDOR_FULL_NAME%3A%22RAYTHEON+COMPANY%22+CONTRACT_FISCAL_YEAR%3A%22",as.character(i),"%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13", sep=""))
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts <- list()
for (j in 1:1) {
df <- foreach(i = 1962+((j-1)*5):1966+((j-1)*5), .combine = 'rbind') %dopar% {
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste('https://www.fpds.gov/ezsearch/fpdsportal?q=GLOBAL_VENDOR_NAME%3A"The+Boeing+Company"+CONTRACT_FISCAL_YEAR%3A%22',as.character(i),'%22&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=22&y=13',sep=""))
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
contracts[[i]] <-df
}
#this is an example of a data scrape request. This is specifically one company and one state over a period of time. My project specifically focused on Arms contractors in MD, but to adapt this to hospitals all I would need to do is
generaldynamicsMD <- foreach(i = 1994:2022, .combine = 'rbind') %dopar% {
library(rvest)
library(dplyr)
readUrl <- function(url) {
out <- tryCatch(
{
read_html(paste(url, sep="")) %>% html_element(xpath='//*[(@id = "csvLink")]') %>% html_attr("href")
},
error=function(cond) {
return(NA)
},
warning=function(cond) {
return(NA)
},
finally={
}
)
return(out)
}
y=readUrl(paste('https://www.fpds.gov/ezsearch/fpdsportal?indexName=awardfull&templateName=1.5.3&s=FPDS.GOV%252525252525252525252525252525252BtemplateName1.5.2%252525252525252525252525252525252BindexNameawardfull&q=ULTIMATE_UEI_NAME%3A%22GENERAL+DYNAMICS%22+CONTRACT_FISCAL_YEAR%3A%22',as.character(i),'%22+VENDOR_ADDRESS_STATE_CODE%3A%22MD%22&x=24&y=13',sep=""))
if (is.na(y)==FALSE) { year_x <- read.csv(paste("https://www.fpds.gov/ezsearch/fpdsportal",y, sep=""))
year_name <- paste("year",i, sep = "")
assign(year_name,year_x) }
}
parallel::stopCluster(cl = my.cluster)
View(generaldynamicsMD)
parallel::stopCluster(cl = my.cluster)
